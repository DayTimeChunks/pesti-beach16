if (MAC) {
if (WIN){
path = file.path("C:/Users/DayTimeChunks/Documents/PhD/HydrologicalMonitoring")
} else {
# path = file.path("/Users/DayTightChunks/Documents/PhD/HydrologicalMonitoring")
path = file.path("/Users/DayTightChunks/Documents/PhD/HydroMonitor/.nosync/HydrologicalMonitoring")
}
} else {
path = file.path("D:/Documents/these_pablo/Alteckendorf2016/HydrologicalMonitoring")
time = read.csv2("D:/Documents/these_pablo/Models/BEACH2016/Analysis/Data/Time.csv")
time$DayMoYr = as.POSIXct(strptime(time$Date, "%d/%m/%Y", tz="EST"))
}
source(file.path(path, "global.R"))
# Plotting functions
library("scales")
library("tidyr")
library("dplyr")
library("reshape")
library("zoo") # na.approx()
# setwd("D:/Documents/these_pablo/Alteckendorf2016/R")
# MAC
# setwd("/Users/DayTightChunks/Documents/PhD/Routput/Alteck/R")
# Mac-WIN
# setwd("C:/Users/DayTightChunks/Documents/Models/pesti-beach16/Analysis/Data")
getwd()
q = read.csv2(file.path(path, "Data/groupAlteck2016_R.csv"))
q$Vol.L = q$Vol2min * 1000
q = q[ , c("Date", "DateCheck", "Q.HW1", "DayMoYr", "Vol.L", "sampleQ", "Type", "SubWeeks", "Weeks", "WeekNo" )]
names(q)
mark = read.csv(file.path(path, "Data/MarkerResponse_R05.csv"))
mark = mark[, c("WeekSubWeek",
# "AveDischarge.m3.h", "Volume.m3",  "Sampled.Hrs",
# "Sampled",
"Conc.mug.L" , "Conc.SD",
# "Vol.SPE.L", "Conc.in500uL",
"OXA_mean", "OXA_SD", "ESA_mean", "ESA_SD",
"N.x", "diss.d13C", "SD.d13C",
"MES.mg.L", "MES.sd", "MO.mg.L", "Conc.Solids.mug.gMES", "Conc.Solids.ug.gMES.SD" #,
#"N.y",  "filt.d13C",  "filt.SD.d13C" #,
#"DD13C.diss", "DD13C.filt"
)]
names(mark)
# Delete repeated W6 observation, or with NA in week markers
# mark = mark[mark$WeekSubWeek != as.character("W6-3j7") & !is.na(mark$WeekSubWeek), ]
q$Date = as.POSIXct(strptime(q$DateCheck, "%d/%m/%Y %H:%M", tz="EST"))
q$DayMoYr = as.POSIXct(strptime(q$DateCheck, "%d/%m/%Y", tz="EST"))
CHECKO = F
if (CHECKO){
sum(is.na(q$Date))
naDates = q[is.na(q$Date == TRUE),]
duplicateAlteck <- q[duplicated(q$DateCheck),]
head(duplicateAlteck)
}
qm = merge(q, mark, by.x = "SubWeeks", by.y = "WeekSubWeek", all = T)
# Dissolved
qm$SmetOut_ug.obs = qm$Vol.L*qm$Conc.mug.L
qm$SmetOut_ug.sd = qm$Vol.L*qm$Conc.SD
qm$OxaOut_ug.obs =  qm$Vol.L*qm$OXA_mean
qm$OxaOut_ug.sd =  qm$Vol.L*qm$OXA_SD
qm$EsaOut_ug.obs =  qm$Vol.L*qm$ESA_mean
qm$EsaOut_ug.sd =  qm$Vol.L*qm$ESA_SD
# Suspended Solids (SS)
# Smet.ug in SS = ug/g * (MES [mg/L] * [1g/10^3mg])  * Vol [L]
qm$SmetSS_ug.obs = qm$Conc.Solids.mug.gMES * (qm$MES.mg.L*1/10^3)  * qm$Vol.L
qm$SmetSS_ug.sd = qm$Conc.Solids.ug.gMES.SD * (qm$MES.sd*1/10^3)  * qm$Vol.L
qm$MassDelta.obs = qm$SmetOut_ug.obs*qm$diss.d13C
qm$MassDelta.sd = qm$SmetOut_ug.sd*qm$SD.d13C
names(qm)
# Step 1
qmDay <- qm %>%
group_by(DayMoYr, SubWeeks) %>%
dplyr::summarize(Volday.L = sum(Vol.L),
SmOut_ug.obs = sum(SmetOut_ug.obs),
SmOut_ug.sd = (sum(SmetOut_ug.sd^2))^0.5, # Cumulative SD
OxOut_ug.obs = sum(OxaOut_ug.obs),
OxOut_ug.sd = (sum(OxaOut_ug.sd^2))^0.5, # Cumulative SD
EsOut_ug.obs = sum(EsaOut_ug.obs),
EsOut_ug.sd = (sum(EsaOut_ug.sd^2))^0.5, # Cumulative SD
ConcSmOut_ugL.obs = SmOut_ug.obs/Volday.L, # Smet
ConcSmOut_ugL.sd = SmOut_ug.sd/Volday.L,
ConcOxOut_ugL.obs = OxOut_ug.obs/Volday.L, # Oxa
ConcOxOut_ugL.sd = OxOut_ug.sd/Volday.L,
ConcEsOut_ugL.obs = EsOut_ug.obs/Volday.L, # Esa
ConcEsOut_ugL.sd = EsOut_ug.sd/Volday.L,
delta.obs = sum(MassDelta.obs)/(sum(SmetOut_ug.obs)),
delta.sd = (sum(MassDelta.sd^2))^0.5/(sum(SmetOut_ug.sd^2))^0.5
)
# Step 2
# Get all duplicated days
allDup = qmDay %>%
group_by(DayMoYr) %>%
filter(n()>1)
# Assume same delta on the same day
deltasDup = allDup %>%
group_by(DayMoYr) %>%
dplyr::summarize(delta.obs = mean(delta.obs, na.rm = T),
delta.sd = mean(delta.sd, na.rm = T))
deltasDup$delta.obs = ifelse(deltasDup$delta.obs == "NaN", NA, deltasDup$delta.obs)
deltasDup$delta.sd = ifelse(deltasDup$delta.sd == "NaN", NA, deltasDup$delta.sd)
# Delete delta columns on allDup
cols = ncol(allDup)-2
allDup = allDup[, c(1:cols)]
# Invert 1st two rows for na.approx
allDup = allDup[c(2,1:nrow(allDup)), ]
allDup$ConcSmOut_ugL.obs = na.approx(allDup$ConcSmOut_ugL.obs)
allDup$ConcSmOut_ugL.sd = na.approx(allDup$ConcSmOut_ugL.sd)
allDup$ConcOxOut_ugL.obs= na.approx(allDup$ConcOxOut_ugL.obs)
allDup$ConcOxOut_ugL.sd = na.approx(allDup$ConcOxOut_ugL.sd)
allDup$ConcEsOut_ugL.obs = na.approx(allDup$ConcEsOut_ugL.obs)
allDup$ConcEsOut_ugL.sd = na.approx(allDup$ConcEsOut_ugL.sd)
allDup = merge(allDup, deltasDup, by = "DayMoYr", all = T)
ndup = qmDay %>%
group_by(DayMoYr) %>%
filter(n()==1)
qmDay = rbind.data.frame(ndup, allDup)
qmDay = qmDay[order(qmDay$DayMoYr), ]
# head(dupQm)
qmBlk = qmDay %>%
group_by(DayMoYr) %>%
dplyr::summarize(VolTot.L = sum(Volday.L),
ConSmOut_ugL.blk = sum(ConcSmOut_ugL.obs * Volday.L)/sum(Volday.L),
ConSmOut_ugL.sd = sum(ConcSmOut_ugL.sd * Volday.L)/sum(Volday.L),
ConOxOut_ugL.blk = sum(ConcOxOut_ugL.obs * Volday.L)/sum(Volday.L),
ConOxOut_ugL.sd = sum(ConcOxOut_ugL.sd * Volday.L)/sum(Volday.L),
ConEsOut_ugL.blk = sum(ConcEsOut_ugL.obs * Volday.L)/sum(Volday.L),
ConEsOut_ugL.sd = sum(ConcEsOut_ugL.sd * Volday.L)/sum(Volday.L),
deltaOut.blk = sum(delta.obs * Volday.L)/sum(Volday.L),
deltaOut.sd =  sum(delta.sd * Volday.L)/sum(Volday.L)
)
m <- q %>%
group_by(DayMoYr) %>%
dplyr::summarise(SubWeeks = SubWeeks[1])
qmBlk = merge(qmBlk, m, by = "DayMoYr")
qmBlk = merge(time, qmBlk, by = "DayMoYr", all = T)
# qmBlk$JDay = seq.int(nrow(qmBlk)) + 176
if (F) {
write.csv(qmBlk, "qmBlk_R.csv", row.names = F) # , sep = ";", dec = ".")
}
names(qmBlk)
View(qmBlk)
Sys.setlocale("LC_ALL", "English")
MAC = F
WIN = F
if (MAC) {
if (WIN){
path = file.path("C:/Users/DayTimeChunks/Documents/PhD/HydrologicalMonitoring")
} else {
# path = file.path("/Users/DayTightChunks/Documents/PhD/HydrologicalMonitoring")
path = file.path("/Users/DayTightChunks/Documents/PhD/HydroMonitor/.nosync/HydrologicalMonitoring")
}
} else {
path = file.path("D:/Documents/these_pablo/Alteckendorf2016/HydrologicalMonitoring")
time = read.csv2("D:/Documents/these_pablo/Models/BEACH2016/Analysis/Data/Time.csv")
time$DayMoYr = as.POSIXct(strptime(time$Date, "%d/%m/%Y", tz="EST"))
}
source(file.path(path, "global.R"))
# Plotting functions
library("scales")
library("tidyr")
library("dplyr")
library("reshape")
library("zoo") # na.approx()
# setwd("D:/Documents/these_pablo/Alteckendorf2016/R")
# MAC
# setwd("/Users/DayTightChunks/Documents/PhD/Routput/Alteck/R")
# Mac-WIN
# setwd("C:/Users/DayTightChunks/Documents/Models/pesti-beach16/Analysis/Data")
getwd()
q = read.csv2(file.path(path, "Data/groupAlteck2016_R.csv"))
q$Vol.L = q$Vol2min * 1000
q = q[ , c("Date", "DateCheck", "Q.HW1", "DayMoYr", "Vol.L", "sampleQ", "Type", "SubWeeks", "Weeks", "WeekNo" )]
names(q)
mark = read.csv(file.path(path, "Data/MarkerResponse_R05.csv"))
mark = mark[, c("WeekSubWeek",
# "AveDischarge.m3.h", "Volume.m3",  "Sampled.Hrs",
# "Sampled",
"Conc.mug.L" , "Conc.SD",
# "Vol.SPE.L", "Conc.in500uL",
"OXA_mean", "OXA_SD", "ESA_mean", "ESA_SD",
"N.x", "diss.d13C", "SD.d13C",
"MES.mg.L", "MES.sd", "MO.mg.L", "Conc.Solids.mug.gMES", "Conc.Solids.ug.gMES.SD" #,
#"N.y",  "filt.d13C",  "filt.SD.d13C" #,
#"DD13C.diss", "DD13C.filt"
)]
names(mark)
# Delete repeated W6 observation, or with NA in week markers
# mark = mark[mark$WeekSubWeek != as.character("W6-3j7") & !is.na(mark$WeekSubWeek), ]
q$Date = as.POSIXct(strptime(q$DateCheck, "%d/%m/%Y %H:%M", tz="EST"))
q$DayMoYr = as.POSIXct(strptime(q$DateCheck, "%d/%m/%Y", tz="EST"))
CHECKO = F
if (CHECKO){
sum(is.na(q$Date))
naDates = q[is.na(q$Date == TRUE),]
duplicateAlteck <- q[duplicated(q$DateCheck),]
head(duplicateAlteck)
}
qm = merge(q, mark, by.x = "SubWeeks", by.y = "WeekSubWeek", all = T)
# Dissolved
qm$SmetOut_ug.obs = qm$Vol.L*qm$Conc.mug.L
qm$SmetOut_ug.sd = qm$Vol.L*qm$Conc.SD
qm$OxaOut_ug.obs =  qm$Vol.L*qm$OXA_mean
qm$OxaOut_ug.sd =  qm$Vol.L*qm$OXA_SD
qm$EsaOut_ug.obs =  qm$Vol.L*qm$ESA_mean
qm$EsaOut_ug.sd =  qm$Vol.L*qm$ESA_SD
# Suspended Solids (SS)
# Smet.ug in SS = ug/g * (MES [mg/L] * [1g/10^3mg])  * Vol [L]
qm$SmetSS_ug.obs = qm$Conc.Solids.mug.gMES * (qm$MES.mg.L*1/10^3)  * qm$Vol.L
qm$SmetSS_ug.sd = qm$Conc.Solids.ug.gMES.SD * (qm$MES.sd*1/10^3)  * qm$Vol.L
qm$MassDelta.obs = qm$SmetOut_ug.obs*qm$diss.d13C
qm$MassDelta.sd = qm$SmetOut_ug.sd*qm$SD.d13C
names(qm)
# Step 1
qmDay <- qm %>%
group_by(DayMoYr, SubWeeks) %>%
dplyr::summarize(Volday.L = sum(Vol.L),
SmOut_ug.obs = sum(SmetOut_ug.obs),
SmOut_ug.sd = (sum(SmetOut_ug.sd^2))^0.5, # Cumulative SD
OxOut_ug.obs = sum(OxaOut_ug.obs),
OxOut_ug.sd = (sum(OxaOut_ug.sd^2))^0.5, # Cumulative SD
EsOut_ug.obs = sum(EsaOut_ug.obs),
EsOut_ug.sd = (sum(EsaOut_ug.sd^2))^0.5, # Cumulative SD
ConcSmOut_ugL.obs = SmOut_ug.obs/Volday.L, # Smet
ConcSmOut_ugL.sd = SmOut_ug.sd/Volday.L,
ConcOxOut_ugL.obs = OxOut_ug.obs/Volday.L, # Oxa
ConcOxOut_ugL.sd = OxOut_ug.sd/Volday.L,
ConcEsOut_ugL.obs = EsOut_ug.obs/Volday.L, # Esa
ConcEsOut_ugL.sd = EsOut_ug.sd/Volday.L,
delta.obs = sum(MassDelta.obs)/(sum(SmetOut_ug.obs)),
delta.sd = (sum(MassDelta.sd^2))^0.5/(sum(SmetOut_ug.sd^2))^0.5
)
# Step 2
# Get all duplicated days
allDup = qmDay %>%
group_by(DayMoYr) %>%
filter(n()>1)
# Assume same delta on the same day
deltasDup = allDup %>%
group_by(DayMoYr) %>%
dplyr::summarize(delta.obs = mean(delta.obs, na.rm = T),
delta.sd = mean(delta.sd, na.rm = T))
deltasDup$delta.obs = ifelse(deltasDup$delta.obs == "NaN", NA, deltasDup$delta.obs)
deltasDup$delta.sd = ifelse(deltasDup$delta.sd == "NaN", NA, deltasDup$delta.sd)
# Delete delta columns on allDup
cols = ncol(allDup)-2
allDup = allDup[, c(1:cols)]
# Invert 1st two rows for na.approx
allDup = allDup[c(2,1:nrow(allDup)), ]
allDup$ConcSmOut_ugL.obs = na.approx(allDup$ConcSmOut_ugL.obs)
allDup$ConcSmOut_ugL.sd = na.approx(allDup$ConcSmOut_ugL.sd)
allDup$ConcOxOut_ugL.obs= na.approx(allDup$ConcOxOut_ugL.obs)
allDup$ConcOxOut_ugL.sd = na.approx(allDup$ConcOxOut_ugL.sd)
allDup$ConcEsOut_ugL.obs = na.approx(allDup$ConcEsOut_ugL.obs)
allDup$ConcEsOut_ugL.sd = na.approx(allDup$ConcEsOut_ugL.sd)
allDup = merge(allDup, deltasDup, by = "DayMoYr", all = T)
ndup = qmDay %>%
group_by(DayMoYr) %>%
filter(n()==1)
qmDay = rbind.data.frame(ndup, allDup)
qmDay = qmDay[order(qmDay$DayMoYr), ]
# head(dupQm)
qmBlk = qmDay %>%
group_by(DayMoYr) %>%
dplyr::summarize(VolTot.L = sum(Volday.L),
ConSmOut_ugL.blk = sum(ConcSmOut_ugL.obs * Volday.L)/sum(Volday.L),
ConSmOut_ugL.sd = sum(ConcSmOut_ugL.sd * Volday.L)/sum(Volday.L),
ConOxOut_ugL.blk = sum(ConcOxOut_ugL.obs * Volday.L)/sum(Volday.L),
ConOxOut_ugL.sd = sum(ConcOxOut_ugL.sd * Volday.L)/sum(Volday.L),
ConEsOut_ugL.blk = sum(ConcEsOut_ugL.obs * Volday.L)/sum(Volday.L),
ConEsOut_ugL.sd = sum(ConcEsOut_ugL.sd * Volday.L)/sum(Volday.L),
deltaOut.blk = sum(delta.obs * Volday.L)/sum(Volday.L),
deltaOut.sd =  sum(delta.sd * Volday.L)/sum(Volday.L)
)
m <- q %>%
group_by(DayMoYr) %>%
dplyr::summarise(SubWeeks = SubWeeks[1])
qmBlk = merge(qmBlk, m, by = "DayMoYr")
qmBlk = merge(time, qmBlk, by = "DayMoYr", all = T)
# qmBlk$JDay = seq.int(nrow(qmBlk)) + 176
if (F) {
write.csv(qmBlk, "qmBlk_R.csv", row.names = F) # , sep = ";", dec = ".")
}
names(qmBlk)
# Vol.L/day
# qmBlk$time = seq.int(nrow(qmBlk))
qmBlk$VolTot.m3 = round(qmBlk$VolTot.L/10^3, 3)
mean(qmBlk$VolTot.m3, na.rm = T)
sd(qmBlk$VolTot.m3, na.rm = T)
Volm3_tss = qmBlk[,c("Jdays", "VolTot.m3")]
Volm3_tss$VolTot.m3 = ifelse(is.na(Volm3_tss$VolTot.m3), -1.0, Volm3_tss$VolTot.m3)
if (F) {
write.table(Volm3_tss, "BEACH_R/q_obs_m3day.tss", sep="\t", row.names = F, col.names = F)
}
if (F) {
## Convert m3.h -> m3
qDay <- q %>%
group_by(DayMoYr) %>%
dplyr::summarize(Q.m3 = sum(Vol2min))
qDay$Q.mm = (qDay$Q.m3/catchment_area)*10^3
qDay$time = seq.int(nrow(qDay))
# Qm3/day
DischQm3_tss = qDay[,c("time", "Q.m3")]
write.table(DischQm3_tss, "BEACH_R/disch_m3day.tss", sep="\t", row.names = F)
# Qmm/day
DischQmm_tss = qDay[,c("time", "Q.mm")]
write.table(DischQmm_tss, "BEACH_R/disch_mmday.tss", sep="\t", row.names = F)
}
Conc_ugL_tss = qmBlk[,c("Jdays", "ConSmOut_ugL.blk")]
View(Conc_ugL_tss)
View(qmBlk)
View(qmDay)
View(qm)
Delta_out_tss = qmBlk[,c("Jdays", "deltaOut.blk")]
View(Delta_out_tss)
if (F){
write.table(Conc_ugL_tss, "BEACH_R/Conc_ugL.tss", sep="\t", row.names = F)
write.table(Delta_out_tss, "BEACH_R/Delta_out.tss", sep="\t", row.names = F)
}
write.table(Conc_ugL_tss, "BEACH_R/Conc_ugL.tss", sep="\t", row.names = F)
write.table(Delta_out_tss, "BEACH_R/Delta_out.tss", sep="\t", row.names = F)
Conc_ugL_tss = qmBlk[,c("Jdays", "ConSmOut_ugL.blk")]
Conc_ugL_tss$ConSmOut_ugL.blk = ifelse(is.na(Conc_ugL_tss$ConSmOut_ugL.blk), 10**9, Conc_ugL_tss$ConSmOut_ugL.blk)
Delta_out_tss = qmBlk[,c("Jdays", "deltaOut.blk")]
Delta_out_tss$deltaOut.blk = ifelse(is.na(Delta_out_tss$deltaOut.blk), 10**9, Delta_out_tss$ConSmOut_ugL.blk)
Delta_out_tss = qmBlk[,c("Jdays", "deltaOut.blk")]
Delta_out_tss$deltaOut.blk = ifelse(is.na(Delta_out_tss$deltaOut.blk), 10**9, Delta_out_tss$deltaOut.blk)
write.table(Conc_ugL_tss, "BEACH_R/Conc_ugL.tss", sep="\t", row.names = F)
write.table(Delta_out_tss, "BEACH_R/Delta_out.tss", sep="\t", row.names = F)
Conc_ugL_tss = qmBlk[,c("Jdays", "ConSmOut_ugL.blk")]
Conc_ugL_tss$ConSmOut_ugL.blk = ifelse(is.na(Conc_ugL_tss$ConSmOut_ugL.blk), -10**9, Conc_ugL_tss$ConSmOut_ugL.blk)
write.table(Conc_ugL_tss, "BEACH_R/Conc_ugL.tss", sep="\t", row.names = F)
write.table(Delta_out_tss, "BEACH_R/Delta_out.tss", sep="\t", row.names = F)
write.table(Conc_ugL_tss, "BEACH_R/Conc_ugL.tss", sep="\t", row.names = F, col.names = F)
write.table(Delta_out_tss, "BEACH_R/Delta_out.tss", sep="\t", row.names = F, col.names = )
View(Conc_ugL_tss)
write.table(Conc_ugL_tss, "BEACH_R/Conc_ugL.tss", sep="\t", row.names = F, col.names = F)
write.table(Delta_out_tss, "BEACH_R/Delta_out.tss", sep="\t", row.names = F, col.names = F)
Sys.setlocale("LC_ALL", "English")
MAC = T
WIN = F
if (MAC) {
if (WIN){
path = file.path("C:/Users/DayTimeChunks/Documents/PhD/HydrologicalMonitoring")
} else {
# path = file.path("/Users/DayTightChunks/Documents/PhD/HydrologicalMonitoring")
path = file.path("/Users/DayTightChunks/Documents/PhD/HydroMonitor/.nosync/HydrologicalMonitoring")
}
} else {
path = file.path("D:/Documents/these_pablo/Alteckendorf2016/HydrologicalMonitoring")
time = read.csv2("D:/Documents/these_pablo/Models/BEACH2016/Analysis/Data/Time.csv")
time$DayMoYr = as.POSIXct(strptime(time$Date, "%d/%m/%Y", tz="EST"))
}
source(file.path(path, "global.R"))
# Plotting functions
library("scales")
library("tidyr")
library("dplyr")
library("reshape")
library("zoo") # na.approx()
# setwd("D:/Documents/these_pablo/Alteckendorf2016/R")
# MAC
# setwd("/Users/DayTightChunks/Documents/PhD/Routput/Alteck/R")
# Mac-WIN
# setwd("C:/Users/DayTightChunks/Documents/Models/pesti-beach16/Analysis/Data")
getwd()
q = read.csv2(file.path(path, "Data/groupAlteck2016_R.csv"))
Sys.setlocale("LC_ALL", "English")
MAC = F
WIN = F
SAVE = F
# Plotting functions
library("scales")
library("tidyr")
library("reshape")
library("zoo") # na.approx()
library("plyr")
library("dplyr")
co = read.csv("DetailConc.csv")
is = read.csv("DetailIsotopes.csv")
View(co)
co = read.csv("DetailConc.csv")
is = read.csv("DetailIsotopes.csv")
if (length(is) == 1) {
is = read.csv("DetailIsotopes.csv", sep = ";")
}
if (length(co) == 1) {
co = read.csv("DetailConc.csv")
}
is = subset(is, !is.na(d13C))
View(is)
co = read.csv("DetailConc.csv")
is = read.csv("DetailIsotopes.csv")
co = read.csv("DetailConc.csv")
is = read.csv("DetailIsotopes.csv")
View(is)
if (length(is) == 1) {
is = read.csv("DetailIsotopes.csv", sep = ";")
}
if (length(co) == 1) {
co = read.csv("DetailConc.csv")
}
View(is)
is = subset(is, !is.na(d13C))
siso <- is[, c("ID", "d13C")]
sumIS = ddply(siso, c("ID"),
summarise,
N_detsoil = length(d13C),
det.d13C = mean(d13C),
det.d13C.SD = sd(d13C)
)
sumIS = subset(sumIS, N_detsoil > 1 & det.d13C.SD < 0.8)
View(sumIS)
ci = merge(co, sumIS, by = "ID", all = T)
ci$DayMoYr = as.POSIXct(strptime(ci$Sampled, "%d/%m/%Y", tz="EST"))
ci = subset(ci, !is.na(ci$DayMoYr))
View(ci)
ci = merge(co, sumIS, by = "ID", all = T)
ci$DayMoYr = as.POSIXct(strptime(ci$Sampled, "%d/%m/%Y", tz="EST"))
ci = subset(ci, !is.na(ci$DayMoYr))
View(ci)
ci$Plot = paste(ci$Transect, ci$Plot, sep = "-")
ci = ci[, c("DayMoYr", "ID", "Plot", "ug.g", "det.d13C", "det.d13C.SD")]
Sys.setlocale("LC_ALL", "English")
SAVE = T # Mode
MAC = F
WIN = F
if (MAC) {
if (WIN){
path = file.path("C:/Users/DayTimeChunks/Documents/PhD/HydrologicalMonitoring")
} else {
# path = file.path("/Users/DayTightChunks/Documents/PhD/HydrologicalMonitoring")
path = file.path("/Users/DayTightChunks/Documents/PhD/hydrological-monitoring")
time = read.csv2("/Users/DayTightChunks/Documents/PhD/Models/phd-model-master/Analysis/Data/Time.csv")
time$DayMoYr = as.POSIXct(strptime(time$Date, "%d/%m/%Y", tz="EST"))
}
} else {
path = file.path("D:/Documents/these_pablo/Alteckendorf2016/HydrologicalMonitoring")
time = read.csv2("D:/Documents/these_pablo/Models/BEACH2016/Analysis/Data/Time.csv")
time$DayMoYr = as.POSIXct(strptime(time$Date, "%d/%m/%Y", tz="EST"))
}
# Lab parameters and field constants
source(file.path(path, "global.R"))
# Plotting functions
library("scales")
library("tidyr")
library("reshape")
library("zoo") # na.approx()
library("plyr")
library("dplyr")
# setwd("D:/Documents/these_pablo/Alteckendorf2016/R")
getwd()
s = read.csv2(file.path(path, "Data/MonitoringScopeSoils_R.csv"))
s$Transect = as.character(s$Transect)
s$Transect = ifelse(s$Transect == 'T', 'V', s$Transect)
s$DayMoYr = as.POSIXct(strptime(s$Date.Soil, "%d/%m/%Y", tz="EST"))
s = subset(s, s$Wnum > 0)
north = subset(s, Transect == "N")
valley = subset(s, Transect == "V")
south = subset(s, Transect == "S")
mean(north$Conc.mug.g.dry.soil, na.rm = T)
mean(valley$Conc.mug.g.dry.soil, na.rm = T)
mean(south$Conc.mug.g.dry.soil, na.rm = T)
mean(north$comp.d13C, na.rm = T)
mean(valley$comp.d13C, na.rm = T)
mean(south$comp.d13C, na.rm = T)
njd = merge(time, north, by = "DayMoYr", all = T)
vjd = merge(time, valley, by = "DayMoYr", all = T)
sjd = merge(time, south, by = "DayMoYr", all = T)
norConc_tss = njd[, c("Jdays", "Conc.mug.g.dry.soil")]
valConc_tss = vjd[, c("Jdays", "Conc.mug.g.dry.soil")]
souConc_tss = sjd[, c("Jdays", "Conc.mug.g.dry.soil")]
norConc_tss$Conc.mug.g.dry.soil = ifelse(is.na(norConc_tss$Conc.mug.g.dry.soil),
-1e9, norConc_tss$Conc.mug.g.dry.soil)
valConc_tss$Conc.mug.g.dry.soil = ifelse(is.na(valConc_tss$Conc.mug.g.dry.soil),
-1e9, valConc_tss$Conc.mug.g.dry.soil)
souConc_tss$Conc.mug.g.dry.soil = ifelse(is.na(souConc_tss$Conc.mug.g.dry.soil),
-1e9, souConc_tss$Conc.mug.g.dry.soil)
norDelta_tss = njd[, c("Jdays", "comp.d13C")]
valDelta_tss = vjd[, c("Jdays", "comp.d13C")]
souDelta_tss = sjd[, c("Jdays", "comp.d13C")]
norDelta_tss$comp.d13C = ifelse(is.na(norDelta_tss$comp.d13C),
1e9, norDelta_tss$comp.d13C)
valDelta_tss$comp.d13C = ifelse(is.na(valDelta_tss$comp.d13C),
1e9, valDelta_tss$comp.d13C)
souDelta_tss$comp.d13C = ifelse(is.na(souDelta_tss$comp.d13C),
1e9, souDelta_tss$comp.d13C)
if (SAVE) {
write.table(njd, "BEACH_R/north.tss", sep="\t", row.names = F)
write.table(vjd, "BEACH_R/valley.tss", sep="\t", row.names = F)
write.table(sjd, "BEACH_R/south.tss", sep="\t", row.names = F)
write.table(norConc_tss, "BEACH_R/northConc.tss", sep="\t", row.names = F)
write.table(valConc_tss, "BEACH_R/valleyConc.tss", sep="\t", row.names = F)
write.table(souConc_tss, "BEACH_R/southConc.tss", sep="\t", row.names = F)
write.table(norDelta_tss, "BEACH_R/northDelta.tss", sep="\t", row.names = F)
write.table(valDelta_tss, "BEACH_R/valleyDelta.tss", sep="\t", row.names = F)
write.table(souDelta_tss, "BEACH_R/southDelta.tss", sep="\t", row.names = F)
}
View(s)
st = merge(time, s, by = "DayMoYr", all = T)
st$IDcal = paste(st$Transect, st$Jdays, sep = "-")
conc_tss = st[, c("Jdays", "Conc.mug.g.dry.soil")]
delta_tss = st[, c("Jdays", "comp.d13C")]
conc_cal = st[, c("Jdays", "Transect", "IDcal", "Conc.mug.g.dry.soil", "Conc.ComSoil.SD")]
names(conc_cal) <- c("Jdays", "Transect", "IDcal", "ug.g", "ug.g.SD")
conc_cal = subset(conc_cal, !is.na(conc_cal$ug.g))
delta_cal = st[, c("Jdays", "Transect", "IDcal", "comp.d13C", "comp.d13C.SD")]
names(delta_cal) <- c("Jdays", "Transect", "IDcal", "d13C", "d13C.SD")
delta_cal = subset(delta_cal, !is.na(delta_cal$d13C))
View(conc_cal)
if (SAVE) {
write.table(conc_cal, "BEACH_R/conc_comp_cal.tss", sep="\t", row.names = F)
write.table(delta_cal, "BEACH_R/delta_comp_cal.tss", sep="\t", row.names = F)
}
